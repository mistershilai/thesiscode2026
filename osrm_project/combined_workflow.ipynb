{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd65fda",
   "metadata": {},
   "source": [
    "# OSRM Project — Combined Workflow\n",
    "\n",
    "This notebook combines the functionality from the `osrm_project` Python scripts into a single, linear notebook you can run start-to-finish.\n",
    "\n",
    "Sections: \n",
    "1. Setup / imports\n",
    "2. Basic facility checks\n",
    "3. (Optional) Build OSRM table-based edges (requires OSRM server)\n",
    "4. Pivot edges into matrices\n",
    "5. Labeling and duration upper-bound\n",
    "6. Analysis & visualizations\n",
    "7. Smoke test example\n",
    "\n",
    "Notes: the notebook will *not* attempt to install packages automatically. If missing packages are reported, run `pip install -r requirements.txt` from the `osrm_project` folder. If you don't have an OSRM instance available, set `run_osrm = False` in the build step and the notebook will skip remote requests and continue where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0fb05a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.11.11 (main, Dec 11 2024, 10:25:04) [Clang 14.0.6 ]\n",
      "Working directory: /Users/elee/Documents/GitHub/thesiscode2026\n",
      "pandas version: 2.3.3\n",
      "Files in current directory: ['nearest_facility.ipynb', 'facilities_with_warehouses.csv', 'priority_antimicrobials_cleaned.numbers', '.DS_Store', 'botswana_population_age_breakdown.csv', 'requirements.txt', 'priority_antimicrobials_estimates.numbers', 'botswanacensusmicrodata.csv', 'duration_matrix.csv', 'facility_id_lookup.csv', 'botswana_geocode', 'README.md', 'repo_trash', '.gitignore', 'osrm_project']\n"
     ]
    }
   ],
   "source": [
    "# Setup: imports and helpers\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from typing import List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Change to the workspace root (where facilities_with_warehouses.csv is located)\n",
    "os.chdir('/Users/elee/Documents/GitHub/thesiscode2026')\n",
    "\n",
    "# simple utility: chunk iterator\n",
    "def chunks(lst, n):\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield i, lst[i:i+n]\n",
    "\n",
    "print(\"Python version:\", sys.version.splitlines()[0])\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"pandas version:\", pd.__version__)\n",
    "print(\"Files in current directory:\", os.listdir('.')[:15])  # List first 15 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ffb7705d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created backup: facilities_with_warehouses.csv.pre_healthpost_fix.20251208T135250.bak\n",
      "Reassigned 18 clinics with \"Health Post\" in name to Health Post\n",
      "  Clinics: 194 → 176\n",
      "  Health Posts: 310 → 328\n",
      "Wrote corrected facilities to facilities_with_warehouses.csv\n"
     ]
    }
   ],
   "source": [
    "# Fix misclassified facilities: Clinics with \"Health Post\" in name should be Health Posts\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "fac_path = 'facilities_with_warehouses.csv'\n",
    "if not os.path.exists(fac_path):\n",
    "    print(f'Warning: {fac_path} not found; skipping Health Post reclassification.')\n",
    "else:\n",
    "    fac = pd.read_csv(fac_path)\n",
    "    \n",
    "    # Find facility name column\n",
    "    name_col = None\n",
    "    for col in fac.columns:\n",
    "        if 'facility' in col.lower() and 'name' in col.lower():\n",
    "            name_col = col\n",
    "            break\n",
    "    \n",
    "    if name_col is None:\n",
    "        print('Warning: Could not find facility name column. Columns:', list(fac.columns))\n",
    "    elif 'Service Delivery Type' not in fac.columns:\n",
    "        print('Warning: Service Delivery Type column not found.')\n",
    "    else:\n",
    "        # Count before\n",
    "        before_clinic = (fac['Service Delivery Type'] == 'Clinic').sum()\n",
    "        before_hp = (fac['Service Delivery Type'] == 'Health Post').sum()\n",
    "        \n",
    "        # Find clinics with \"Health Post\" in name and reassign\n",
    "        mask = (fac['Service Delivery Type'] == 'Clinic') & (fac[name_col].str.contains('Health Post', case=False, na=False))\n",
    "        reassigned_count = mask.sum()\n",
    "        \n",
    "        if reassigned_count > 0:\n",
    "            fac.loc[mask, 'Service Delivery Type'] = 'Health Post'\n",
    "            \n",
    "            # Create backup before writing\n",
    "            backup_path = fac_path + f'.pre_healthpost_fix.{datetime.now().strftime(\"%Y%m%dT%H%M%S\")}.bak'\n",
    "            shutil.copy(fac_path, backup_path)\n",
    "            print(f'Created backup: {backup_path}')\n",
    "            \n",
    "            # Write corrected file\n",
    "            fac.to_csv(fac_path, index=False)\n",
    "            \n",
    "            # Report changes\n",
    "            after_clinic = (fac['Service Delivery Type'] == 'Clinic').sum()\n",
    "            after_hp = (fac['Service Delivery Type'] == 'Health Post').sum()\n",
    "            print(f'Reassigned {reassigned_count} clinics with \"Health Post\" in name to Health Post')\n",
    "            print(f'  Clinics: {before_clinic} → {after_clinic}')\n",
    "            print(f'  Health Posts: {before_hp} → {after_hp}')\n",
    "            print(f'Wrote corrected facilities to {fac_path}')\n",
    "        else:\n",
    "            print('No clinics with \"Health Post\" in name found; no changes needed.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "96f8c2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['Old Facility Code', 'New Facility Code', 'Facility Name', 'District', 'DHMT', 'Latitude', 'Longitude', 'Telephone', 'Service Delivery Type', 'Facility Owner', 'Facility Status', 'Is_Warehouse']\n",
      "Rows: 636\n",
      "Nulls in Latitude/Longitude: 0 0\n",
      "Warehouses: 22\n",
      "Facilities: 614\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Old Facility Code</th>\n",
       "      <th>New Facility Code</th>\n",
       "      <th>Facility Name</th>\n",
       "      <th>District</th>\n",
       "      <th>DHMT</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Telephone</th>\n",
       "      <th>Service Delivery Type</th>\n",
       "      <th>Facility Owner</th>\n",
       "      <th>Facility Status</th>\n",
       "      <th>Is_Warehouse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8/3/10</td>\n",
       "      <td>972228-1</td>\n",
       "      <td>Airstrip Clinic</td>\n",
       "      <td>Mahalapye</td>\n",
       "      <td>Mahalapye</td>\n",
       "      <td>-23.115351</td>\n",
       "      <td>26.824311</td>\n",
       "      <td>+267 4710009/74166629</td>\n",
       "      <td>Clinic</td>\n",
       "      <td>GOVERNMENT</td>\n",
       "      <td>OPERATIONAL</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16-4-02</td>\n",
       "      <td>713823-3</td>\n",
       "      <td>Area L Health Post</td>\n",
       "      <td>Greater Francistown</td>\n",
       "      <td>Greater Francistown</td>\n",
       "      <td>-21.160365</td>\n",
       "      <td>27.517292</td>\n",
       "      <td>+267 2470046</td>\n",
       "      <td>Health Post</td>\n",
       "      <td>GOVERNMENT</td>\n",
       "      <td>OPERATIONAL</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16-4-03</td>\n",
       "      <td>611993-7</td>\n",
       "      <td>Area S Health Post</td>\n",
       "      <td>Greater Francistown</td>\n",
       "      <td>Greater Francistown</td>\n",
       "      <td>-21.156493</td>\n",
       "      <td>27.497482</td>\n",
       "      <td>+267 2470047</td>\n",
       "      <td>Health Post</td>\n",
       "      <td>GOVERNMENT</td>\n",
       "      <td>OPERATIONAL</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9/3/01</td>\n",
       "      <td>153397-5</td>\n",
       "      <td>Artesia Clinic</td>\n",
       "      <td>Kgatleng</td>\n",
       "      <td>Kgatleng</td>\n",
       "      <td>-24.013836</td>\n",
       "      <td>26.320154</td>\n",
       "      <td>+267 5729710</td>\n",
       "      <td>Clinic with Maternity</td>\n",
       "      <td>GOVERNMENT</td>\n",
       "      <td>OPERATIONAL</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7879798</td>\n",
       "      <td>419322-3</td>\n",
       "      <td>BAC Clinic</td>\n",
       "      <td>Greater Gaborone</td>\n",
       "      <td>Greater Gaborone</td>\n",
       "      <td>-24.680000</td>\n",
       "      <td>25.920000</td>\n",
       "      <td>+267 3953062</td>\n",
       "      <td>Clinic</td>\n",
       "      <td>GOVERNMENT</td>\n",
       "      <td>OPERATIONAL</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Old Facility Code New Facility Code       Facility Name  \\\n",
       "0            8/3/10          972228-1     Airstrip Clinic   \n",
       "1           16-4-02          713823-3  Area L Health Post   \n",
       "2           16-4-03          611993-7  Area S Health Post   \n",
       "3            9/3/01          153397-5      Artesia Clinic   \n",
       "4           7879798          419322-3          BAC Clinic   \n",
       "\n",
       "              District                 DHMT   Latitude  Longitude  \\\n",
       "0            Mahalapye            Mahalapye -23.115351  26.824311   \n",
       "1  Greater Francistown  Greater Francistown -21.160365  27.517292   \n",
       "2  Greater Francistown  Greater Francistown -21.156493  27.497482   \n",
       "3             Kgatleng             Kgatleng -24.013836  26.320154   \n",
       "4     Greater Gaborone     Greater Gaborone -24.680000  25.920000   \n",
       "\n",
       "               Telephone  Service Delivery Type Facility Owner  \\\n",
       "0  +267 4710009/74166629                 Clinic     GOVERNMENT   \n",
       "1           +267 2470046            Health Post     GOVERNMENT   \n",
       "2           +267 2470047            Health Post     GOVERNMENT   \n",
       "3           +267 5729710  Clinic with Maternity     GOVERNMENT   \n",
       "4           +267 3953062                 Clinic     GOVERNMENT   \n",
       "\n",
       "  Facility Status  Is_Warehouse  \n",
       "0     OPERATIONAL         False  \n",
       "1     OPERATIONAL         False  \n",
       "2     OPERATIONAL         False  \n",
       "3     OPERATIONAL         False  \n",
       "4     OPERATIONAL         False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1) Basic facility checks (adapted from check_facilities.py)\n",
    "FAC_CSV = 'facilities_with_warehouses.csv'\n",
    "if not os.path.exists(FAC_CSV):\n",
    "    print(f'File not found: {FAC_CSV} — please place it in the workspace root.')\n",
    "else:\n",
    "    fac = pd.read_csv(FAC_CSV)\n",
    "    print('Columns:', list(fac.columns))\n",
    "    print('Rows:', len(fac))\n",
    "    if 'Latitude' in fac.columns and 'Longitude' in fac.columns:\n",
    "        print('Nulls in Latitude/Longitude:', fac['Latitude'].isna().sum(), fac['Longitude'].isna().sum())\n",
    "    else:\n",
    "        print('No Latitude/Longitude columns found; please check your CSV.')\n",
    "    if 'Is_Warehouse' in fac.columns:\n",
    "        print('Warehouses:', (fac['Is_Warehouse'] == True).sum())\n",
    "        print('Facilities:', (fac['Is_Warehouse'] == False).sum())\n",
    "    display(fac.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdf404d",
   "metadata": {},
   "source": [
    "## 2) Optional: Build edges with OSRM Table API\n",
    "\n",
    "This step calls an OSRM `/table` request to compute pairwise durations/distances between warehouse sources and facility destinations.\n",
    "Set `run_osrm = True` and ensure `osrm_url` points to your OSRM instance (e.g., `http://localhost:5001`). If OSRM is unavailable the cell will skip requests and return an empty DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fc48ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Docker helper: prepare and run a local OSRM container using the local PBF file\n",
    "# This helper is conservative: it checks for `docker`, validates the PBF exists in the notebook folder,\n",
    "# runs the extract/partition/customize steps, starts `osrm-routed` in detached mode and polls a tiny table request\n",
    "# NOTE: the helper mounts the current notebook folder into /data in the container and will create .osrm files there.\n",
    "import os, shutil, subprocess, time, requests\n",
    "\n",
    "def start_osrm_docker(pbf='botswana-latest.osm.pbf', mount_dir='.', image='osrm/osrm-backend:latest', profile='/opt/car.lua', host_port=5001, use_mld=True, timeout=120):\n",
    "    \"\"\"Start OSRM in Docker using the provided PBF located under `mount_dir` on the host.\"\"\"\n",
    "    # check docker availability\n",
    "    if shutil.which('docker') is None:\n",
    "        print('Docker is not available on PATH. Install Docker Desktop and ensure `docker` is runnable.')\n",
    "        return None\n",
    "    pbf_path = os.path.join(mount_dir, pbf)\n",
    "    if not os.path.exists(pbf_path):\n",
    "        print(f'PBF not found at {pbf_path}. Place the file in the notebook folder or set mount_dir appropriately.')\n",
    "        return None\n",
    "\n",
    "    abs_mount = os.path.abspath(mount_dir)\n",
    "    # build command sequence: extract -> partition/customize (MLD) or extract -> contract (CH) -> run routed\n",
    "    steps = []\n",
    "    steps.append(['docker','run','--rm','-v', f'{abs_mount}:/data', image, 'osrm-extract','-p', profile, f'/data/{pbf}'])\n",
    "    if use_mld:\n",
    "        steps.append(['docker','run','--rm','-v', f'{abs_mount}:/data', image, 'osrm-partition', f'/data/{pbf}.osrm'])\n",
    "        steps.append(['docker','run','--rm','-v', f'{abs_mount}:/data', image, 'osrm-customize', f'/data/{pbf}.osrm'])\n",
    "        run_cmd = ['docker','run','-d','-p', f'{host_port}:5000','-v', f'{abs_mount}:/data', image, 'osrm-routed','--algorithm','mld', f'/data/{pbf}.osrm']\n",
    "    else:\n",
    "        steps.append(['docker','run','--rm','-v', f'{abs_mount}:/data', image, 'osrm-contract', f'/data/{pbf}.osrm'])\n",
    "        run_cmd = ['docker','run','-d','-p', f'{host_port}:5000','-v', f'{abs_mount}:/data', image, 'osrm-routed', f'/data/{pbf}.osrm']\n",
    "\n",
    "    # run the preparatory steps\n",
    "    try:\n",
    "        for c in steps:\n",
    "            print('Running:', ' '.join(c))\n",
    "            subprocess.run(c, check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print('A preparatory docker step failed:', e)\n",
    "        return None\n",
    "\n",
    "    # start osrm-routed in detached mode\n",
    "    print('Starting osrm-routed (detached)...')\n",
    "    try:\n",
    "        cid = subprocess.check_output(run_cmd).decode().strip()\n",
    "        print('Started container id:', cid)\n",
    "    except Exception as e:\n",
    "        print('Failed to start osrm-routed container:', e)\n",
    "        return None\n",
    "\n",
    "    # basic health check: small table query using the container host_port\n",
    "    url = f'http://localhost:{host_port}/table/v1/driving/0,0;0,0'\n",
    "    params = {'sources':'0','destinations':'1','annotations':'duration'}\n",
    "    deadline = time.time() + timeout\n",
    "    while time.time() < deadline:\n",
    "        try:\n",
    "            r = requests.get(url, params=params, timeout=5)\n",
    "            if r.ok and r.json().get('code') == 'Ok':\n",
    "                print('OSRM appears up and responding on port', host_port)\n",
    "                return cid\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(2)\n",
    "    print('OSRM did not respond within timeout. Check container logs with `docker logs <container_id>`.')\n",
    "    return cid\n",
    "\n",
    "# Example: call start_osrm_docker() to run the pipeline (this will create .osrm files in the notebook folder).\n",
    "# cid = start_osrm_docker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "75c4a068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_edges_all_pairs(csv_path='facilities_with_warehouses.csv', osrm_url='http://localhost:5001', chunk=30, limit=0, run_osrm=True):\n",
    "    \"\"\"Build edges between ALL facilities (all-pairs distance matrix).\n",
    "    Uses chunking for both sources and destinations to avoid URL length limits.\n",
    "    Returns a DataFrame with columns: source_id, source_name, dest_id, dest_name, distance_m, duration_s\n",
    "    \"\"\"\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(csv_path)\n",
    "    \n",
    "    df = pd.read_csv(csv_path).dropna(subset=['Latitude','Longitude']).copy()\n",
    "    \n",
    "    if limit and limit > 0:\n",
    "        df = df.head(limit).copy()\n",
    "    \n",
    "    def make_id(s: pd.DataFrame) -> pd.Series:\n",
    "        a = s.get('New Facility Code') if 'New Facility Code' in s.columns else None\n",
    "        b = s.get('Old Facility Code') if 'Old Facility Code' in s.columns else None\n",
    "        idx_series = s.reset_index().index.astype(str)\n",
    "        if a is None and b is None:\n",
    "            return idx_series\n",
    "        if a is not None and b is not None:\n",
    "            out = a.combine_first(b)\n",
    "        elif a is not None:\n",
    "            out = a.copy()\n",
    "        else:\n",
    "            out = b.copy()\n",
    "        out = out.where(out.notna(), idx_series)\n",
    "        return out.astype(str)\n",
    "    \n",
    "    df['node_id'] = make_id(df)\n",
    "    df['label'] = df.get('Facility Name', df.get('facility_name', pd.Series(['']*len(df)))).astype(str)\n",
    "    df['coord'] = df.apply(lambda r: f\"{r['Longitude']},{r['Latitude']}\", axis=1)\n",
    "    \n",
    "    n_total = len(df)\n",
    "    print(f\"Total facilities: {n_total}\")\n",
    "    print(f\"Using chunk size: {chunk}\")\n",
    "    \n",
    "    base_url = f'{osrm_url}/table/v1/driving/'\n",
    "    edges = []\n",
    "    \n",
    "    if not run_osrm:\n",
    "        print('run_osrm is False — skipping remote OSRM requests. You can set run_osrm=True to contact OSRM.')\n",
    "        return pd.DataFrame(columns=['source_id','source_name','dest_id','dest_name','distance_m','duration_s'])\n",
    "    \n",
    "    # Chunk both sources and destinations to avoid URL length limits\n",
    "    total_chunks = 0\n",
    "    for src_offset, src_chunk in chunks(df, chunk):\n",
    "        src_coords = src_chunk['coord'].tolist()\n",
    "        n_src = len(src_coords)\n",
    "        \n",
    "        for dst_offset, dst_chunk in chunks(df, chunk):\n",
    "            total_chunks += 1\n",
    "            dst_coords = dst_chunk['coord'].tolist()\n",
    "            coords = ';'.join(src_coords + dst_coords)\n",
    "            \n",
    "            sources = ';'.join(map(str, range(n_src)))\n",
    "            destinations = ';'.join(map(str, range(n_src, n_src + len(dst_coords))))\n",
    "            params = {'sources': sources, 'destinations': destinations, 'annotations': 'duration,distance'}\n",
    "            \n",
    "            url = base_url + coords\n",
    "            try:\n",
    "                r = requests.get(url, params=params, timeout=300)\n",
    "                r.raise_for_status()\n",
    "                data = r.json()\n",
    "            except Exception as e:\n",
    "                print(f'[chunk {total_chunks}] request failed at src offset {src_offset}, dst offset {dst_offset}: {e}')\n",
    "                continue\n",
    "            \n",
    "            if data.get('code') != 'Ok':\n",
    "                print(f'[chunk {total_chunks}] OSRM error: {data.get(\"message\", data)}')\n",
    "                continue\n",
    "            \n",
    "            dists = data.get('distances')\n",
    "            durs  = data.get('durations')\n",
    "            if dists is None or durs is None:\n",
    "                print(f'[chunk {total_chunks}] missing distances/durations')\n",
    "                continue\n",
    "            \n",
    "            for si, (s_id, s_name) in enumerate(zip(src_chunk['node_id'], src_chunk['label'])):\n",
    "                for di, (d_id, d_name) in enumerate(zip(dst_chunk['node_id'], dst_chunk['label'])):\n",
    "                    dist = dists[si][di]\n",
    "                    dur  = durs[si][di]\n",
    "                    if dist is None or dur is None:\n",
    "                        continue\n",
    "                    edges.append((s_id, s_name, d_id, d_name, dist, dur))\n",
    "            \n",
    "            if total_chunks % 10 == 0:\n",
    "                print(f'[{total_chunks} chunks] processed {min(src_offset + n_src, n_total)} × {min(dst_offset + len(dst_chunk), n_total)} facility pairs')\n",
    "    \n",
    "    edges_df = pd.DataFrame(edges, columns=['source_id','source_name','dest_id','dest_name','distance_m','duration_s'])\n",
    "    print(f'Total chunks: {total_chunks}, total edges: {len(edges_df)}')\n",
    "    return edges_df\n",
    "\n",
    "# To test: uncomment below and run this cell\n",
    "# edges_df = build_edges_all_pairs(run_osrm=True)\n",
    "# print('edges_df shape:', edges_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa5866",
   "metadata": {},
   "source": [
    "## 3) Pivot edges into distance/duration matrices (adapted from `pivot_matrices.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b9aecea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No edges to pivot — returning empty matrices\n",
      "dist_mat shape: None\n"
     ]
    }
   ],
   "source": [
    "def pivot_edges_to_matrices(edges_df, out_prefix=''):\n",
    "    \"\"\"Take a long-form edges DataFrame and write / return wide distance and duration matrices.\n",
    "    Expects columns: source_id, source_name, dest_id, dest_name, distance_m, duration_s\n",
    "    For all-pairs: creates a symmetric matrix with facility names as both rows and columns.\n",
    "    Also writes ID-indexed versions and a lookup table.\n",
    "    \"\"\"\n",
    "    if edges_df is None or len(edges_df) == 0:\n",
    "        print('No edges to pivot — returning empty matrices')\n",
    "        return None, None\n",
    "\n",
    "    # drop duplicates keeping smallest distance per pair\n",
    "    df = edges_df.sort_values('distance_m').drop_duplicates(subset=['source_id','dest_id'], keep='first').copy()\n",
    "\n",
    "    # Pivot by IDs first\n",
    "    dist_by_id = df.pivot(index='dest_id', columns='source_id', values='distance_m')\n",
    "    dur_by_id  = df.pivot(index='dest_id', columns='source_id', values='duration_s')\n",
    "\n",
    "    # Create ID-to-name mapping\n",
    "    src_names = df[['source_id','source_name']].drop_duplicates().set_index('source_id')['source_name']\n",
    "    dst_names = df[['dest_id','dest_name']].drop_duplicates().set_index('dest_id')['dest_name']\n",
    "    \n",
    "    # Merge name mappings (prefer source names but use dest names as fallback)\n",
    "    id_to_name = pd.concat([src_names, dst_names]).drop_duplicates(keep='first')\n",
    "\n",
    "    # Create named versions: replace index and columns with facility names\n",
    "    dist_named = dist_by_id.copy()\n",
    "    dist_named.index = dist_named.index.map(id_to_name)\n",
    "    dist_named.columns = dist_named.columns.map(id_to_name)\n",
    "    \n",
    "    dur_named = dur_by_id.copy()\n",
    "    dur_named.index = dur_named.index.map(id_to_name)\n",
    "    dur_named.columns = dur_named.columns.map(id_to_name)\n",
    "\n",
    "    # Write outputs to disk if out_prefix is provided\n",
    "    if out_prefix is not None and out_prefix != '':\n",
    "        try:\n",
    "            os.makedirs(out_prefix, exist_ok=True)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Write ID-indexed versions\n",
    "        dist_by_id.to_csv(os.path.join(out_prefix, 'distance_matrix.csv'))\n",
    "        dur_by_id.to_csv(os.path.join(out_prefix, 'duration_matrix.csv'))\n",
    "        \n",
    "        # Write named versions\n",
    "        dist_named.to_csv(os.path.join(out_prefix, 'distance_matrix_named.csv'))\n",
    "        dur_named.to_csv(os.path.join(out_prefix, 'duration_matrix_named.csv'))\n",
    "        \n",
    "        # Write lookup table\n",
    "        id_to_name.to_csv(os.path.join(out_prefix, 'facility_id_lookup.csv'), header=['source_name'])\n",
    "        \n",
    "        print('Wrote pivoted matrices to disk with prefix', out_prefix)\n",
    "    else:\n",
    "        # Write to current directory if no prefix\n",
    "        dist_by_id.to_csv('distance_matrix.csv')\n",
    "        dur_by_id.to_csv('duration_matrix.csv')\n",
    "        dist_named.to_csv('distance_matrix_named.csv')\n",
    "        dur_named.to_csv('duration_matrix_named.csv')\n",
    "        id_to_name.to_csv('facility_id_lookup.csv', header=['source_name'])\n",
    "        print('Wrote pivoted matrices to disk (current directory)')\n",
    "\n",
    "    # Return the named versions (with facility names as index/columns)\n",
    "    return dist_named, dur_named\n",
    "\n",
    "# When edges_df is empty (because run_osrm=False) this will be a no-op\n",
    "dist_mat, dur_mat = pivot_edges_to_matrices(edges_df)\n",
    "print('dist_mat shape:', None if dist_mat is None else dist_mat.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c99730f",
   "metadata": {},
   "source": [
    "## 4) Label matrices and apply duration upper-bound (adapted from `label_and_upperbound.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5372b970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled shapes: (614, 22) (614, 22)\n"
     ]
    }
   ],
   "source": [
    "def label_and_upperbound(fac_csv='facilities_with_warehouses.csv', dist_df=None, dur_df=None, out_prefix=''):\n",
    "    \"\"\"If provided with named (index/columns=facility names) dataframes, label them with service type and apply 1.2× upper bound to duration.\n",
    "    If dataframes are None, the function will attempt to load `distance_matrix_named.csv` and `duration_matrix_named.csv` from disk.\n",
    "    \"\"\"\n",
    "    fac = None\n",
    "    if os.path.exists(fac_csv):\n",
    "        fac = pd.read_csv(fac_csv)\n",
    "        category_map = fac.set_index('Facility Name')['Service Delivery Type'].to_dict()\n",
    "    else:\n",
    "        category_map = {}\n",
    "\n",
    "    if dist_df is None or dur_df is None:\n",
    "        # try to load named matrices if present\n",
    "        if os.path.exists('distance_matrix_named.csv') and os.path.exists('duration_matrix_named.csv'):\n",
    "            dist_df = pd.read_csv('distance_matrix_named.csv', index_col=0)\n",
    "            dur_df = pd.read_csv('duration_matrix_named.csv', index_col=0)\n",
    "        else:\n",
    "            print('No named matrices provided or on disk — skipping labeling step')\n",
    "            return None, None\n",
    "\n",
    "    dur_upper = dur_df * 1.2\n",
    "\n",
    "    def apply_labels(df):\n",
    "        return [f\"{name} ({category_map.get(name, '--')})\" for name in df]\n",
    "\n",
    "    dur_upper.index = apply_labels(dur_upper.index)\n",
    "    dur_upper.columns = apply_labels(dur_upper.columns)\n",
    "    dist_df.index = apply_labels(dist_df.index)\n",
    "    dist_df.columns = apply_labels(dist_df.columns)\n",
    "\n",
    "    if out_prefix:\n",
    "        dur_upper.to_csv(out_prefix + 'duration_matrix_upperbound_labeled.csv')\n",
    "        dist_df.to_csv(out_prefix + 'distance_matrix_labeled.csv')\n",
    "        print('Wrote labeled matrices with prefix', out_prefix)\n",
    "\n",
    "    return dist_df, dur_upper\n",
    "\n",
    "# Try to label if pivot produced matrices; otherwise this is a no-op\n",
    "labeled_dist, labeled_dur = label_and_upperbound(dist_df=dist_mat, dur_df=dur_mat)\n",
    "print('labeled shapes:', None if labeled_dist is None else labeled_dist.shape, None if labeled_dur is None else labeled_dur.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6edef8",
   "metadata": {},
   "source": [
    "## 5) Analysis & visualizations (adapted from `matrixanalysis.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0d36fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_labels(labels):\n",
    "    from collections import Counter\n",
    "    counts = Counter()\n",
    "    new_labels = []\n",
    "    for lbl in labels:\n",
    "        counts[lbl] += 1\n",
    "        if counts[lbl] > 1:\n",
    "            new_labels.append(f\"{lbl} ({counts[lbl]})\")\n",
    "        else:\n",
    "            new_labels.append(lbl)\n",
    "    return new_labels\n",
    "\n",
    "def analyze(labeled_dist_csv='distance_matrix_labeled.csv', labeled_dur_csv='duration_matrix_upperbound_labeled.csv', fac_csv='facilities_with_warehouses.csv', out_dir=''):\n",
    "    if not os.path.exists(labeled_dist_csv) or not os.path.exists(labeled_dur_csv):\n",
    "        print('Labeled matrices not found on disk — skipping analysis (provide labeled CSVs).')\n",
    "        return\n",
    "\n",
    "    dist = pd.read_csv(labeled_dist_csv, index_col=0)\n",
    "    dur = pd.read_csv(labeled_dur_csv, index_col=0)\n",
    "    fac = pd.read_csv(fac_csv) if os.path.exists(fac_csv) else pd.DataFrame()\n",
    "\n",
    "    # ensure unique labels\n",
    "    if dist.index.duplicated().any() or dist.columns.duplicated().any():\n",
    "        print('Warning: Duplicate facility names found — disambiguating.')\n",
    "        dist.index = deduplicate_labels(dist.index)\n",
    "        dist.columns = deduplicate_labels(dist.columns)\n",
    "        dur.index = dist.index\n",
    "        dur.columns = dist.columns\n",
    "\n",
    "    dist_np = dist.apply(pd.to_numeric, errors='coerce').to_numpy(dtype=float)\n",
    "    dur_np = dur.apply(pd.to_numeric, errors='coerce').to_numpy(dtype=float)\n",
    "\n",
    "    # Asymmetry analysis: only compute if matrix is square; for rectangular matrices, skip\n",
    "    if dist_np.shape[0] == dist_np.shape[1]:\n",
    "        dist_asym_np = np.abs(dist_np - dist_np.T)\n",
    "        dur_asym_np = np.abs(dur_np - dur_np.T)\n",
    "        dist_asym_np = np.nan_to_num(dist_asym_np, nan=0.0)\n",
    "        dur_asym_np = np.nan_to_num(dur_asym_np, nan=0.0)\n",
    "        \n",
    "        summary = {\n",
    "            'mean_dist_asym_m': np.mean(dist_asym_np),\n",
    "            'max_dist_asym_m': np.max(dist_asym_np),\n",
    "            'mean_dur_asym_s': np.mean(dur_asym_np),\n",
    "            'max_dur_asym_s': np.max(dur_asym_np),\n",
    "            'n_facilities': dist.shape[0],\n",
    "        }\n",
    "    else:\n",
    "        # Rectangular matrix: skip asymmetry, just report basic stats\n",
    "        print(f'Note: Matrix is rectangular ({dist_np.shape[0]} × {dist_np.shape[1]}), skipping asymmetry analysis.')\n",
    "        valid = (dist_np > 0) & (dur_np > 0)\n",
    "        summary = {\n",
    "            'mean_dist_m': np.nanmean(dist_np[valid]) if np.any(valid) else np.nan,\n",
    "            'max_dist_m': np.nanmax(dist_np[valid]) if np.any(valid) else np.nan,\n",
    "            'mean_dur_s': np.nanmean(dur_np[valid]) if np.any(valid) else np.nan,\n",
    "            'max_dur_s': np.nanmax(dur_np[valid]) if np.any(valid) else np.nan,\n",
    "            'n_sources': dist.shape[0],\n",
    "            'n_destinations': dist.shape[1],\n",
    "        }\n",
    "    \n",
    "    # Save summary CSV to the output directory\n",
    "    summary_path = os.path.join(out_dir, 'matrix_summary.csv') if out_dir else 'matrix_summary.csv'\n",
    "    pd.DataFrame([summary]).to_csv(summary_path, index=False)\n",
    "    print(f'Wrote {summary_path}')\n",
    "\n",
    "    # facility type counts visualization if possible\n",
    "    if not fac.empty and 'Service Delivery Type' in fac.columns:\n",
    "        type_counts = fac['Service Delivery Type'].value_counts().sort_values(ascending=False)\n",
    "        print('\\n' + '='*60)\n",
    "        print('FACILITY COUNTS BY SERVICE DELIVERY TYPE')\n",
    "        print('='*60)\n",
    "        for service_type, count in type_counts.items():\n",
    "            print(f\"  {service_type:40s} : {count:4d}\")\n",
    "        print('='*60 + '\\n')\n",
    "        \n",
    "        # Save counts to CSV\n",
    "        counts_df = pd.DataFrame({'Service Delivery Type': type_counts.index, 'Count': type_counts.values})\n",
    "        counts_path = os.path.join(out_dir, 'facility_type_counts.csv') if out_dir else 'facility_type_counts.csv'\n",
    "        counts_df.to_csv(counts_path, index=False)\n",
    "        print(f'Wrote {counts_path}')\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(8,4))\n",
    "        sns.barplot(y=type_counts.index, x=type_counts.values, palette='viridis')\n",
    "        plt.title('Number of Facilities by Service Delivery Type')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Facility Type')\n",
    "        plt.tight_layout()\n",
    "        png_path = os.path.join(out_dir, 'facility_type_counts.png') if out_dir else 'facility_type_counts.png'\n",
    "        plt.savefig(png_path)\n",
    "        plt.close()\n",
    "        print(f'Wrote {png_path}')\n",
    "\n",
    "    # asymmetry distribution (only for square matrices)\n",
    "    if dist_np.shape[0] == dist_np.shape[1]:\n",
    "        flat_asym = dist_asym_np[np.triu_indices_from(dist_asym_np, k=1)]\n",
    "        plt.figure(figsize=(7,4))\n",
    "        sns.histplot(flat_asym/1000, bins=50, color='coral', kde=True)\n",
    "        plt.xlim(0,15)\n",
    "        plt.xlabel('Asymmetry (km)')\n",
    "        plt.tight_layout()\n",
    "        png_path = os.path.join(out_dir, 'asymmetry_distribution.png') if out_dir else 'asymmetry_distribution.png'\n",
    "        plt.savefig(png_path)\n",
    "        plt.close()\n",
    "        print(f'Wrote {png_path}')\n",
    "\n",
    "    # implied speeds\n",
    "    valid = (dist_np > 0) & (dur_np > 0)\n",
    "    speed_mps = (dist_np[valid] / dur_np[valid]).flatten()\n",
    "    plt.figure(figsize=(7,4))\n",
    "    sns.histplot(speed_mps*3.6, bins=40, color='seagreen')\n",
    "    plt.xlabel('Implied Travel Speed (km/h)')\n",
    "    plt.tight_layout()\n",
    "    png_path = os.path.join(out_dir, 'speed_distribution.png') if out_dir else 'speed_distribution.png'\n",
    "    plt.savefig(png_path)\n",
    "    plt.close()\n",
    "    print(f'Wrote {png_path}')\n",
    "    print('Done analysis')\n",
    "\n",
    "# Not running analyze() automatically — call analyze() when you have labeled matrices on disk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c1b80",
   "metadata": {},
   "source": [
    "## 6) Smoke test example (the small OSRM table check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "841ab897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smoke_test_table(csv='facilities_with_warehouses.csv', osrm_url='http://localhost:5001'):\n",
    "    # replicates matrix_smoke_test.py behavior for a tiny 2x5 example\n",
    "    if not os.path.exists(csv):\n",
    "        print('Facilities CSV not found — cannot run smoke test.')\n",
    "        return\n",
    "    df = pd.read_csv(csv).dropna(subset=['Latitude','Longitude']).copy()\n",
    "    src = df[df['Is_Warehouse'] == True].copy()\n",
    "    dst = df[df['Is_Warehouse'] == False].copy()\n",
    "    src_small = src.head(2).reset_index(drop=True)\n",
    "    dst_small = dst.head(5).reset_index(drop=True)\n",
    "    if len(src_small) == 0 or len(dst_small) == 0:\n",
    "        print('Not enough sources/destinations for smoke test')\n",
    "        return\n",
    "    def fmt_coord(row):\n",
    "        return f\"{row['Longitude']},{row['Latitude']}\"\n",
    "    coords = [fmt_coord(r) for _, r in pd.concat([src_small, dst_small]).iterrows()]\n",
    "    n_src = len(src_small)\n",
    "    url = f'{osrm_url}/table/v1/driving/' + ';'.join(coords)\n",
    "    params = {'sources': ';'.join(map(str, range(n_src))), 'destinations': ';'.join(map(str, range(n_src, n_src + len(dst_small)))), 'annotations': 'duration,distance'}\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "    except Exception as e:\n",
    "        print('OSRM request failed (is OSRM running?):', e)\n",
    "        return\n",
    "    if data.get('code') != 'Ok':\n",
    "        print('OSRM returned error:', data)\n",
    "        return\n",
    "    durs = data.get('durations')\n",
    "    dists = data.get('distances')\n",
    "    if dists is not None:\n",
    "        dist = pd.DataFrame(dists, index=src_small['Facility Name'].tolist(), columns=dst_small['Facility Name'].tolist())\n",
    "        display(dist.round(1))\n",
    "    if durs is not None:\n",
    "        dur = pd.DataFrame(durs, index=src_small['Facility Name'].tolist(), columns=dst_small['Facility Name'].tolist())\n",
    "        display(dur.round(1))\n",
    "\n",
    "# To run the smoke test, call smoke_test_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977cf303",
   "metadata": {},
   "source": [
    "## Next steps & usage\n",
    "\n",
    "- To compute full matrices using your local OSRM server: set `run_osrm=True` in the `build_edges_from_facilities()` call in the \n",
    " cell and run that cell.\n",
    "- After `edges_df` is produced, run the pivot cell to create `distance_matrix.csv` and `duration_matrix.csv`.\n",
    "- Run the labeling cell to produce labeled matrices and apply the 1.2× duration upper-bound.\n",
    "- Call `analyze()` (from the Analysis cell) once labeled matrices are present to produce summary CSV and PNGs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03bb893",
   "metadata": {},
   "source": [
    "## 7) Full pipeline: Run all steps end-to-end\n",
    "\n",
    "Execute this cell to regenerate all matrices and CSVs from the OSRM server. This will take several minutes depending on facility count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ce7f092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FULL PIPELINE: REGENERATE ALL MATRICES AND CSVS\n",
      "======================================================================\n",
      "\n",
      "[1/5] Loading facilities...\n",
      "  Loaded 636 facilities\n",
      "[2/5] Building edges from OSRM (this may take many minutes)...\n",
      "  Note: Chunking both sources and destinations to stay within URL limits\n",
      "Total facilities: 636\n",
      "Using chunk size: 30\n",
      "[10 chunks] processed 30 × 300 facility pairs\n",
      "[10 chunks] processed 30 × 300 facility pairs\n",
      "[20 chunks] processed 30 × 600 facility pairs\n",
      "[20 chunks] processed 30 × 600 facility pairs\n",
      "[30 chunks] processed 60 × 240 facility pairs\n",
      "[30 chunks] processed 60 × 240 facility pairs\n",
      "[40 chunks] processed 60 × 540 facility pairs\n",
      "[40 chunks] processed 60 × 540 facility pairs\n",
      "[50 chunks] processed 90 × 180 facility pairs\n",
      "[50 chunks] processed 90 × 180 facility pairs\n",
      "[60 chunks] processed 90 × 480 facility pairs\n",
      "[60 chunks] processed 90 × 480 facility pairs\n",
      "[70 chunks] processed 120 × 120 facility pairs\n",
      "[70 chunks] processed 120 × 120 facility pairs\n",
      "[80 chunks] processed 120 × 420 facility pairs\n",
      "[80 chunks] processed 120 × 420 facility pairs\n",
      "[90 chunks] processed 150 × 60 facility pairs\n",
      "[90 chunks] processed 150 × 60 facility pairs\n",
      "[100 chunks] processed 150 × 360 facility pairs\n",
      "[100 chunks] processed 150 × 360 facility pairs\n",
      "[110 chunks] processed 150 × 636 facility pairs\n",
      "[110 chunks] processed 150 × 636 facility pairs\n",
      "[120 chunks] processed 180 × 300 facility pairs\n",
      "[120 chunks] processed 180 × 300 facility pairs\n",
      "[130 chunks] processed 180 × 600 facility pairs\n",
      "[130 chunks] processed 180 × 600 facility pairs\n",
      "[140 chunks] processed 210 × 240 facility pairs\n",
      "[140 chunks] processed 210 × 240 facility pairs\n",
      "[150 chunks] processed 210 × 540 facility pairs\n",
      "[150 chunks] processed 210 × 540 facility pairs\n",
      "[160 chunks] processed 240 × 180 facility pairs\n",
      "[160 chunks] processed 240 × 180 facility pairs\n",
      "[170 chunks] processed 240 × 480 facility pairs\n",
      "[170 chunks] processed 240 × 480 facility pairs\n",
      "[180 chunks] processed 270 × 120 facility pairs\n",
      "[180 chunks] processed 270 × 120 facility pairs\n",
      "[190 chunks] processed 270 × 420 facility pairs\n",
      "[190 chunks] processed 270 × 420 facility pairs\n",
      "[200 chunks] processed 300 × 60 facility pairs\n",
      "[200 chunks] processed 300 × 60 facility pairs\n",
      "[210 chunks] processed 300 × 360 facility pairs\n",
      "[210 chunks] processed 300 × 360 facility pairs\n",
      "[220 chunks] processed 300 × 636 facility pairs\n",
      "[220 chunks] processed 300 × 636 facility pairs\n",
      "[230 chunks] processed 330 × 300 facility pairs\n",
      "[230 chunks] processed 330 × 300 facility pairs\n",
      "[240 chunks] processed 330 × 600 facility pairs\n",
      "[240 chunks] processed 330 × 600 facility pairs\n",
      "[250 chunks] processed 360 × 240 facility pairs\n",
      "[250 chunks] processed 360 × 240 facility pairs\n",
      "[260 chunks] processed 360 × 540 facility pairs\n",
      "[260 chunks] processed 360 × 540 facility pairs\n",
      "[270 chunks] processed 390 × 180 facility pairs\n",
      "[270 chunks] processed 390 × 180 facility pairs\n",
      "[280 chunks] processed 390 × 480 facility pairs\n",
      "[280 chunks] processed 390 × 480 facility pairs\n",
      "[290 chunks] processed 420 × 120 facility pairs\n",
      "[290 chunks] processed 420 × 120 facility pairs\n",
      "[300 chunks] processed 420 × 420 facility pairs\n",
      "[300 chunks] processed 420 × 420 facility pairs\n",
      "[310 chunks] processed 450 × 60 facility pairs\n",
      "[310 chunks] processed 450 × 60 facility pairs\n",
      "[320 chunks] processed 450 × 360 facility pairs\n",
      "[320 chunks] processed 450 × 360 facility pairs\n",
      "[330 chunks] processed 450 × 636 facility pairs\n",
      "[330 chunks] processed 450 × 636 facility pairs\n",
      "[340 chunks] processed 480 × 300 facility pairs\n",
      "[340 chunks] processed 480 × 300 facility pairs\n",
      "[350 chunks] processed 480 × 600 facility pairs\n",
      "[350 chunks] processed 480 × 600 facility pairs\n",
      "[360 chunks] processed 510 × 240 facility pairs\n",
      "[360 chunks] processed 510 × 240 facility pairs\n",
      "[370 chunks] processed 510 × 540 facility pairs\n",
      "[370 chunks] processed 510 × 540 facility pairs\n",
      "[380 chunks] processed 540 × 180 facility pairs\n",
      "[380 chunks] processed 540 × 180 facility pairs\n",
      "[390 chunks] processed 540 × 480 facility pairs\n",
      "[390 chunks] processed 540 × 480 facility pairs\n",
      "[400 chunks] processed 570 × 120 facility pairs\n",
      "[400 chunks] processed 570 × 120 facility pairs\n",
      "[410 chunks] processed 570 × 420 facility pairs\n",
      "[410 chunks] processed 570 × 420 facility pairs\n",
      "[420 chunks] processed 600 × 60 facility pairs\n",
      "[420 chunks] processed 600 × 60 facility pairs\n",
      "[430 chunks] processed 600 × 360 facility pairs\n",
      "[430 chunks] processed 600 × 360 facility pairs\n",
      "[440 chunks] processed 600 × 636 facility pairs\n",
      "[440 chunks] processed 600 × 636 facility pairs\n",
      "[450 chunks] processed 630 × 300 facility pairs\n",
      "[450 chunks] processed 630 × 300 facility pairs\n",
      "[460 chunks] processed 630 × 600 facility pairs\n",
      "[460 chunks] processed 630 × 600 facility pairs\n",
      "[470 chunks] processed 636 × 240 facility pairs\n",
      "[470 chunks] processed 636 × 240 facility pairs\n",
      "[480 chunks] processed 636 × 540 facility pairs\n",
      "[480 chunks] processed 636 × 540 facility pairs\n",
      "Total chunks: 484, total edges: 404496\n",
      "  Generated 404496 edges\n",
      "[3/5] Pivoting edges into distance/duration matrices...\n",
      "Total chunks: 484, total edges: 404496\n",
      "  Generated 404496 edges\n",
      "[3/5] Pivoting edges into distance/duration matrices...\n",
      "Wrote pivoted matrices to disk with prefix osrm_project/\n",
      "  Distance matrix shape: (636, 636)\n",
      "  Duration matrix shape: (636, 636)\n",
      "[4/5] Labeling matrices and applying 1.2× duration upper-bound...\n",
      "Wrote pivoted matrices to disk with prefix osrm_project/\n",
      "  Distance matrix shape: (636, 636)\n",
      "  Duration matrix shape: (636, 636)\n",
      "[4/5] Labeling matrices and applying 1.2× duration upper-bound...\n",
      "Wrote labeled matrices with prefix osrm_project/\n",
      "  Labeled distance matrix shape: (636, 636)\n",
      "  Labeled duration (upper-bound) matrix shape: (636, 636)\n",
      "[5/5] Running analysis (creating summary CSV and visualizations)...\n",
      "Warning: Duplicate facility names found — disambiguating.\n",
      "Wrote labeled matrices with prefix osrm_project/\n",
      "  Labeled distance matrix shape: (636, 636)\n",
      "  Labeled duration (upper-bound) matrix shape: (636, 636)\n",
      "[5/5] Running analysis (creating summary CSV and visualizations)...\n",
      "Warning: Duplicate facility names found — disambiguating.\n",
      "Wrote osrm_project/matrix_summary.csv\n",
      "\n",
      "============================================================\n",
      "FACILITY COUNTS BY SERVICE DELIVERY TYPE\n",
      "============================================================\n",
      "  Health Post                              :  328\n",
      "  Clinic                                   :  176\n",
      "  Clinic with Maternity                    :   82\n",
      "  Warehouse                                :   22\n",
      "  Primary Hospital                         :   15\n",
      "  District Hospital                        :   10\n",
      "  Referral Hospital                        :    3\n",
      "============================================================\n",
      "\n",
      "Wrote osrm_project/facility_type_counts.csv\n",
      "Wrote osrm_project/facility_type_counts.png\n",
      "Wrote osrm_project/matrix_summary.csv\n",
      "\n",
      "============================================================\n",
      "FACILITY COUNTS BY SERVICE DELIVERY TYPE\n",
      "============================================================\n",
      "  Health Post                              :  328\n",
      "  Clinic                                   :  176\n",
      "  Clinic with Maternity                    :   82\n",
      "  Warehouse                                :   22\n",
      "  Primary Hospital                         :   15\n",
      "  District Hospital                        :   10\n",
      "  Referral Hospital                        :    3\n",
      "============================================================\n",
      "\n",
      "Wrote osrm_project/facility_type_counts.csv\n",
      "Wrote osrm_project/facility_type_counts.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0n/fdwzy91j2j38zqkrjnq3yc040000gn/T/ipykernel_22365/2302127342.py:83: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(y=type_counts.index, x=type_counts.values, palette='viridis')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote osrm_project/asymmetry_distribution.png\n",
      "Wrote osrm_project/speed_distribution.png\n",
      "Done analysis\n",
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Generated files in osrm_project/:\n",
      "  ✓ distance_matrix.csv                                (3.5 MB)\n",
      "  ✓ duration_matrix.csv                                (3.0 MB)\n",
      "  ✓ distance_matrix_named.csv                          (3.5 MB)\n",
      "  ✓ duration_matrix_named.csv                          (3.0 MB)\n",
      "  ✓ facility_id_lookup.csv                             (0.0 MB)\n",
      "  ✓ distance_matrix_labeled.csv                        (3.5 MB)\n",
      "  ✓ duration_matrix_upperbound_labeled.csv             (4.5 MB)\n",
      "  ✓ matrix_summary.csv                                 (0.0 MB)\n",
      "  ✓ facility_type_counts.csv                           (0.0 MB)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FULL PIPELINE: REGENERATE ALL MATRICES AND CSVS\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "\n",
    "# Step 1: Load facilities\n",
    "print(\"[1/5] Loading facilities...\")\n",
    "fac = pd.read_csv('facilities_with_warehouses.csv')\n",
    "print(f\"  Loaded {len(fac)} facilities\")\n",
    "\n",
    "# Step 2: Build edges from OSRM (all-pairs: every facility to every facility)\n",
    "print(\"[2/5] Building edges from OSRM (this may take many minutes)...\")\n",
    "print(\"  Note: Chunking both sources and destinations to stay within URL limits\")\n",
    "edges_df = build_edges_all_pairs(run_osrm=True, osrm_url='http://localhost:5001', chunk=30, limit=0)\n",
    "print(f\"  Generated {len(edges_df)} edges\")\n",
    "\n",
    "# Step 3: Pivot to matrices\n",
    "print(\"[3/5] Pivoting edges into distance/duration matrices...\")\n",
    "dist_mat, dur_mat = pivot_edges_to_matrices(edges_df, out_prefix='osrm_project/')\n",
    "if dist_mat is not None:\n",
    "    print(f\"  Distance matrix shape: {dist_mat.shape}\")\n",
    "    print(f\"  Duration matrix shape: {dur_mat.shape}\")\n",
    "else:\n",
    "    print(\"  WARNING: No matrices produced\")\n",
    "\n",
    "# Step 4: Label matrices\n",
    "print(\"[4/5] Labeling matrices and applying 1.2× duration upper-bound...\")\n",
    "labeled_dist, labeled_dur = label_and_upperbound(fac_csv='facilities_with_warehouses.csv', dist_df=dist_mat, dur_df=dur_mat, out_prefix='osrm_project/')\n",
    "if labeled_dist is not None:\n",
    "    print(f\"  Labeled distance matrix shape: {labeled_dist.shape}\")\n",
    "    print(f\"  Labeled duration (upper-bound) matrix shape: {labeled_dur.shape}\")\n",
    "else:\n",
    "    print(\"  WARNING: No labeled matrices produced\")\n",
    "\n",
    "# Step 5: Analysis\n",
    "print(\"[5/5] Running analysis (creating summary CSV and visualizations)...\")\n",
    "analyze(\n",
    "    labeled_dist_csv='osrm_project/distance_matrix_labeled.csv',\n",
    "    labeled_dur_csv='osrm_project/duration_matrix_upperbound_labeled.csv',\n",
    "    fac_csv='facilities_with_warehouses.csv',\n",
    "    out_dir='osrm_project'\n",
    ")\n",
    "\n",
    "print()\n",
    "print(\"=\" * 70)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print()\n",
    "print(\"Generated files in osrm_project/:\")\n",
    "for fname in ['distance_matrix.csv', 'duration_matrix.csv', 'distance_matrix_named.csv', 'duration_matrix_named.csv', 'facility_id_lookup.csv', 'distance_matrix_labeled.csv', 'duration_matrix_upperbound_labeled.csv', 'matrix_summary.csv', 'facility_type_counts.csv']:\n",
    "    path = os.path.join('osrm_project', fname)\n",
    "    if os.path.exists(path):\n",
    "        size_mb = os.path.getsize(path) / (1024**2)\n",
    "        print(f\"  ✓ {fname:50s} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ✗ {fname:50s} (NOT FOUND)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amr_opt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
